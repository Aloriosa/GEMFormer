{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef826a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data from https://www.dropbox.com/s/npidmtadreo6df2/data.zip to \n",
    "import json\n",
    "train = json.load(open('./2wikimultihopqa/train.json', 'r'))\n",
    "dev = json.load(open('./2wikimultihopqa/dev.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ed6290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prepro_char_based_targets import process_file\n",
    "from datasets.arrow_dataset import Dataset\n",
    "\n",
    "\n",
    "train_examples = process_file(train, with_special_seps=True, dataset_name='2wikimhqa')\n",
    "train_examples = Dataset.from_dict({feature: [train_examples[i][feature] for i in range(len(train_examples))] for feature in train_examples[0]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2c2086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(train_examples, '../2wikimhqa_train_examples_with_special_seps.pkl')\n",
    "\n",
    "val_examples = process_file(dev, with_special_seps=True, dataset_name='2wikimhqa')\n",
    "val_examples = Dataset.from_dict({feature: [val_examples[i][feature] for i in range(len(val_examples))] for feature in val_examples[0]})\n",
    "torch.save(val_examples, '../2wikimhqa_val_examples_with_special_seps.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "060f1314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle gemformer imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6b2502f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 16:15:02.278868: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-29 16:15:03.249341: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial vocab len = 50265\n",
      "We have added 5 tokens\n",
      "final vocab len = 50270\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import RobertaTokenizerFast, default_data_collator\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets.arrow_dataset import Dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from gemformer.utils import add_qa_evidence_tokens, pad_and_drop_duplicates, ROBERTA_BASE_SPECIAL_TOKENS\n",
    "\n",
    "tokenizer_name = 'roberta-base'\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_name)\n",
    "tokenizer = add_qa_evidence_tokens(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1443d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = torch.load('../2wikimhqa_train_examples_with_special_seps.pkl')\n",
    "\n",
    "stride = 20\n",
    "max_num_answers = 64\n",
    "max_num_paragraphs = 10 # by dataset construction\n",
    "max_num_sentences = 210 # from train and val data\n",
    "eos = tokenizer.convert_tokens_to_ids(tokenizer.eos_token) #'</s>'\n",
    "TITLE_END_token = tokenizer.convert_tokens_to_ids('</t>') # indicating the end of the title of a paragraph\n",
    "TITLE_START_token = tokenizer.convert_tokens_to_ids('<t>')\n",
    "SENT_MARKER_END_token = tokenizer.convert_tokens_to_ids('[/sent]')\n",
    "MAX_SEQ_LEN = 512\n",
    "\n",
    "def preprocess_roberta_long_training_examples(train_examples):\n",
    "    questions = [q.strip() for q in train_examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        train_examples['context'],\n",
    "        max_length=MAX_SEQ_LEN,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    raw_ids = [train_examples['id'][kk] for kk in inputs['overflow_to_sample_mapping']]\n",
    "    # store start/end positions of context to filter part of sequence for uncertainty-based topk\n",
    "    inputs.update({\"id\": train_examples['id']})\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = train_examples['char_answer_offsets']\n",
    "    batch_question_type = [] # yes = 0, no = 1, span = 2\n",
    "    ex_context_start_id = []\n",
    "    ex_context_end_id = []\n",
    "    batch_start_positions_list = []\n",
    "    batch_end_positions_list = []\n",
    "    supp_sents = train_examples['supp_sent_char_offsets']\n",
    "    supp_titles = train_examples['supp_title_char_offsets']\n",
    "    batch_title_positions = []\n",
    "    batch_sent_positions = []\n",
    "    batch_titles_to_sents = []\n",
    "\n",
    "    for ex_id in train_examples[\"id\"]:\n",
    "        ex_indices = np.where(np.array(raw_ids) == ex_id)[0].tolist()\n",
    "        context_token_ids = [np.where(np.array(inputs['input_ids'][i]) == eos)[0] for i in ex_indices]\n",
    "        ex_context_start_id.append([elem[1] + 1 for elem in context_token_ids])\n",
    "        ex_context_end_id.append([elem[-1] for elem in context_token_ids])\n",
    "\n",
    "        start_positions_list = []\n",
    "        end_positions_list = []\n",
    "        question_type = []\n",
    "        supp_sent_start_positions = []\n",
    "        supp_sent_end_positions = []\n",
    "        supp_title_start_positions = []\n",
    "        supp_title_end_positions = []\n",
    "\n",
    "        for ex_sample in ex_indices:\n",
    "            offset_idx = ex_sample\n",
    "            offset = offset_mapping[offset_idx]\n",
    "            sample_idx = sample_map[offset_idx]\n",
    "            sequence_ids = inputs.sequence_ids(offset_idx)\n",
    "            # Find the start and end of the context\n",
    "            idx = 0\n",
    "            while sequence_ids[idx] != 1:\n",
    "                idx += 1\n",
    "            context_start = idx\n",
    "            while sequence_ids[idx] == 1:\n",
    "                idx += 1\n",
    "            context_end = idx - 1\n",
    "\n",
    "            ques_type_start_char = answers[sample_idx][0][0]\n",
    "            ques_type_end_char = answers[sample_idx][0][1]\n",
    "            if ques_type_start_char == -1 and ques_type_end_char == -1:\n",
    "                question_type.append(0)\n",
    "            elif ques_type_start_char == -2 and ques_type_end_char == -2:\n",
    "                question_type.append(1)\n",
    "            else:\n",
    "                question_type.append(2)\n",
    "\n",
    "            start_positions = []\n",
    "            end_positions = []\n",
    "\n",
    "            for answer in answers[sample_idx]:\n",
    "                start_char = answer[0]\n",
    "                end_char = answer[1]\n",
    "\n",
    "                if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "                    continue\n",
    "                else:\n",
    "                    idx = context_start\n",
    "                    while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                        idx += 1\n",
    "                    start_positions.append(idx - 1)\n",
    "\n",
    "                    idx = context_end\n",
    "                    while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                        idx -= 1\n",
    "                    end_positions.append(idx + 1)\n",
    "\n",
    "            start_positions, end_positions = pad_and_drop_duplicates(start_positions, end_positions, max_num_answers)\n",
    "            start_positions_list.append(start_positions)\n",
    "            end_positions_list.append(end_positions)\n",
    "\n",
    "            supp_title_start_positions.append([])\n",
    "            supp_title_end_positions.append([])\n",
    "\n",
    "            for supp_title_idx in range(len(supp_titles[sample_idx])):\n",
    "                supp_title = supp_titles[sample_idx][supp_title_idx]\n",
    "                supp_title_start_char = supp_title[0]\n",
    "                supp_title_end_char = supp_title[1]\n",
    "\n",
    "                if offset[context_start][0] > supp_title_start_char or offset[context_end][1] < supp_title_end_char:\n",
    "                    continue\n",
    "                else:\n",
    "                    idx1 = context_start\n",
    "                    while idx1 <= context_end and offset[idx1][0] <= supp_title_start_char:\n",
    "                        idx1 += 1\n",
    "                    supp_title_start_positions[-1].append(idx1 - 1)\n",
    "\n",
    "                    idx1 = context_end\n",
    "                    while idx1 >= context_start and offset[idx1][1] >= supp_title_end_char:\n",
    "                        idx1 -= 1\n",
    "                    supp_title_end_positions[-1].append(idx1 + 1)\n",
    "\n",
    "            supp_sent_start_positions.append([])\n",
    "            supp_sent_end_positions.append([])\n",
    "            for supp_sent_idx in range(len(supp_sents[sample_idx])):\n",
    "                supp_sent = supp_sents[sample_idx][supp_sent_idx]\n",
    "                supp_sent_start_char = supp_sent[0]\n",
    "                supp_sent_end_char = supp_sent[1]\n",
    "\n",
    "                if offset[context_start][0] > supp_sent_start_char or offset[context_end][1] < supp_sent_end_char:\n",
    "                    continue\n",
    "                else:\n",
    "                    idx2 = context_start\n",
    "                    while idx2 <= context_end and offset[idx2][0] <= supp_sent_start_char:\n",
    "                        idx2 += 1\n",
    "                    supp_sent_start_positions[-1].append(idx2 - 1)\n",
    "\n",
    "                    idx2 = context_end\n",
    "                    while idx2 >= context_start and offset[idx2][1] >= supp_sent_end_char:\n",
    "                        idx2 -= 1\n",
    "                    supp_sent_end_positions[-1].append(idx2 + 1)\n",
    "\n",
    "        title_positions = []\n",
    "        for i, supp in zip([inputs['input_ids'][ii] for ii in ex_indices], supp_title_start_positions):\n",
    "            title_positions.append([0 if j not in supp else 1 for j in np.where(np.array(i) == TITLE_START_token)[0]])\n",
    "            title_positions[-1] = pad_and_drop_duplicates(start_positions=title_positions[-1], \n",
    "                                                          max_num_answers=max_num_paragraphs)\n",
    "        sent_positions = []\n",
    "        for i, supp in zip([inputs['input_ids'][ii] for ii in ex_indices], supp_sent_end_positions):\n",
    "            sent_positions.append([0 if j not in supp else 1 for j in np.where(np.array(i) == SENT_MARKER_END_token)[0]])\n",
    "            sent_positions[-1] = pad_and_drop_duplicates(start_positions=sent_positions[-1], \n",
    "                                                         max_num_answers=max_num_sentences)\n",
    "\n",
    "        batch_question_type.append(question_type)\n",
    "        batch_start_positions_list.append(start_positions_list)\n",
    "        batch_end_positions_list.append(end_positions_list)\n",
    "        batch_title_positions.append(title_positions)\n",
    "        batch_sent_positions.append(sent_positions)\n",
    "\n",
    "    inputs.update({\"start_positions\": batch_start_positions_list})\n",
    "    inputs.update({\"end_positions\": batch_end_positions_list})\n",
    "    inputs.update({'question_type': batch_question_type})\n",
    "    inputs.update({'context_start_id': ex_context_start_id})\n",
    "    inputs.update({'context_end_id': ex_context_end_id})\n",
    "    inputs.update({\"supp_title_labels\": batch_title_positions})\n",
    "    inputs.update({\"supp_sent_labels\": batch_sent_positions})\n",
    "\n",
    "    rearranged_inps = []\n",
    "    rearranged_masks = []\n",
    "\n",
    "    for ex_id in train_examples[\"id\"]:\n",
    "        ex_indices = np.where(np.array(raw_ids) == ex_id)[0].tolist()\n",
    "        rearranged_inps.append([inputs['input_ids'][i] for i in ex_indices])\n",
    "        rearranged_masks.append([inputs['attention_mask'][i] for i in ex_indices])\n",
    "\n",
    "    inputs.update({'input_ids': rearranged_inps})\n",
    "    inputs.update({'attention_mask': rearranged_masks})\n",
    "\n",
    "    return inputs\n",
    "\n",
    "\n",
    "mem_train_dataset = train_examples.map(\n",
    "    preprocess_roberta_long_training_examples,\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    remove_columns=train_examples.column_names,\n",
    ")\n",
    "\n",
    "mem_train_dataset.save_to_disk('../2wikimhqa_preprocessed_train_examples_512_multitask_stride20_one_doc_batched_without_zero_answer_pos_without_CoT_triplets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d98afdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba486954",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_examples = torch.load('../2wikimhqa_val_examples_with_special_seps.pkl')\n",
    "\n",
    "def preprocess_longformer_validation_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_SEQ_LEN,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    # store raw sample id to match subsamples related to the same big context document \n",
    "    raw_ids = [examples['id'][kk] for kk in inputs['overflow_to_sample_mapping']]\n",
    "    inputs.update({\"example_ids\": examples['id']})\n",
    "    offset_mapping = inputs[\"offset_mapping\"]\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    ex_context_start_id = []\n",
    "    ex_context_end_id = []\n",
    "    supp_sents = examples['supp_sent_char_offsets']\n",
    "    supp_titles = examples['supp_title_char_offsets']\n",
    "    batch_title_positions = []\n",
    "    batch_sent_positions = []\n",
    "    batch_titles_to_sents = []\n",
    "    batch_concat_titles_ids = []\n",
    "\n",
    "    for ex_id in examples[\"id\"]:\n",
    "        ex_indices = np.where(np.array(raw_ids) == ex_id)[0].tolist()\n",
    "        context_token_ids = [np.where(np.array(inputs['input_ids'][i]) == eos)[0] for i in ex_indices]\n",
    "        ex_context_start_id.append([elem[1] + 1 for elem in context_token_ids])\n",
    "        ex_context_end_id.append([elem[-1] for elem in context_token_ids])\n",
    "\n",
    "        supp_sent_start_positions = []\n",
    "        supp_sent_end_positions = []\n",
    "        supp_title_start_positions = []\n",
    "        supp_title_end_positions = []\n",
    "\n",
    "        for ex_sample in ex_indices:\n",
    "            offset_idx = ex_sample\n",
    "            offset = offset_mapping[offset_idx]\n",
    "            sample_idx = sample_map[offset_idx]\n",
    "            sequence_ids = inputs.sequence_ids(offset_idx)\n",
    "\n",
    "            inputs[\"offset_mapping\"][offset_idx] = [\n",
    "                o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "            ]\n",
    "            # Find the start and end of the context\n",
    "            idx = 0\n",
    "            while sequence_ids[idx] != 1:\n",
    "                idx += 1\n",
    "            context_start = idx\n",
    "            while sequence_ids[idx] == 1:\n",
    "                idx += 1\n",
    "            context_end = idx - 1\n",
    "\n",
    "            supp_title_start_positions.append([])\n",
    "            supp_title_end_positions.append([])\n",
    "            for supp_title_idx in range(len(supp_titles[sample_idx])):\n",
    "                supp_title = supp_titles[sample_idx][supp_title_idx]\n",
    "                supp_title_start_char = supp_title[0]\n",
    "                supp_title_end_char = supp_title[1]\n",
    "                if offset[context_start][0] > supp_title_start_char or offset[context_end][1] < supp_title_end_char:\n",
    "                    continue\n",
    "                else:\n",
    "                    idx1 = context_start\n",
    "                    while idx1 <= context_end and offset[idx1][0] <= supp_title_start_char:\n",
    "                        idx1 += 1\n",
    "                    supp_title_start_positions[-1].append(idx1 - 1)\n",
    "\n",
    "                    idx1 = context_end\n",
    "                    while idx1 >= context_start and offset[idx1][1] >= supp_title_end_char:\n",
    "                        idx1 -= 1\n",
    "                    supp_title_end_positions[-1].append(idx1 + 1)\n",
    "\n",
    "            supp_sent_start_positions.append([])\n",
    "            supp_sent_end_positions.append([])\n",
    "            for supp_sent_idx in range(len(supp_sents[sample_idx])):\n",
    "                supp_sent = supp_sents[sample_idx][supp_sent_idx]\n",
    "                supp_sent_start_char = supp_sent[0]\n",
    "                supp_sent_end_char = supp_sent[1]\n",
    "                if offset[context_start][0] > supp_sent_start_char or offset[context_end][1] < supp_sent_end_char:\n",
    "                    continue\n",
    "                else:\n",
    "                    idx2 = context_start\n",
    "                    while idx2 <= context_end and offset[idx2][0] <= supp_sent_start_char:\n",
    "                        idx2 += 1\n",
    "                    supp_sent_start_positions[-1].append(idx2 - 1)\n",
    "\n",
    "                    idx2 = context_end\n",
    "                    while idx2 >= context_start and offset[idx2][1] >= supp_sent_end_char:\n",
    "                        idx2 -= 1\n",
    "                    supp_sent_end_positions[-1].append(idx2 + 1)\n",
    "\n",
    "        title_positions = []\n",
    "        for i, supp in zip([inputs['input_ids'][ii] for ii in ex_indices], supp_title_start_positions):\n",
    "            title_positions.append([0 if j not in supp else 1 for j in np.where(np.array(i) == TITLE_START_token)[0]])\n",
    "            title_positions[-1] = pad_and_drop_duplicates(start_positions=title_positions[-1], \n",
    "                                                          max_num_answers=max_num_paragraphs)\n",
    "        sent_positions = []\n",
    "        for i, supp in zip([inputs['input_ids'][ii] for ii in ex_indices], supp_sent_end_positions):\n",
    "            sent_positions.append([0 if j not in supp else 1 for j in np.where(np.array(i) == SENT_MARKER_END_token)[0]])\n",
    "            sent_positions[-1] = pad_and_drop_duplicates(start_positions=sent_positions[-1], \n",
    "                                                         max_num_answers=max_num_sentences)\n",
    "\n",
    "        concat_input_ids = []\n",
    "        concat_offset_mapping = []\n",
    "        for ii in ex_indices:\n",
    "            concat_input_ids += inputs['input_ids'][ii]\n",
    "            concat_offset_mapping += inputs['offset_mapping'][ii]\n",
    "\n",
    "        titles = np.where(np.array(concat_input_ids) == TITLE_START_token)[0]\n",
    "        np_titles_concat_offset_mapping = np.array(concat_offset_mapping)[titles]\n",
    "        titles_offsets_unique = sorted(np.unique(np_titles_concat_offset_mapping.tolist(), axis=0).tolist())\n",
    "        concat_titles_ids = [titles_offsets_unique.index(list(i)) for i in np_titles_concat_offset_mapping]\n",
    "     \n",
    "        global_sents = np.where(np.array(concat_input_ids) == SENT_MARKER_END_token)[0]\n",
    "        global_sents_offsets_unique = []\n",
    "        for i in np.array(concat_offset_mapping)[global_sents]:\n",
    "            if i not in global_sents_offsets_unique:\n",
    "                global_sents_offsets_unique.append(i)\n",
    "\n",
    "        titles_to_sents = []\n",
    "\n",
    "        for i, offsets_list in zip([inputs['input_ids'][ii] for ii in ex_indices], \n",
    "                                   [inputs['offset_mapping'][ii] for ii in ex_indices]):\n",
    "            sents = np.where(np.array(i) == SENT_MARKER_END_token)[0].tolist()\n",
    "            sents_offsets = np.array(offsets_list)[sents]\n",
    "\n",
    "            tmp = []\n",
    "            if len(titles_offsets_unique) > 1:\n",
    "                #local chunk sent_id, global doc sent_id\n",
    "                tmp = [[(sent_id, global_sents_offsets_unique.index(sent_offset)) for sent_id, sent_offset in enumerate(sents_offsets) if ((sent_offset[0] >= title_offset[-1]) and (sent_offset[-1] <= titles_offsets_unique[i+1][0])) ] for i, title_offset in enumerate(titles_offsets_unique[:-1])]\n",
    "\n",
    "            if len(titles_offsets_unique) > 0:\n",
    "                tmp.append([(sent_id, global_sents_offsets_unique.index(sent_offset)) for sent_id, sent_offset in enumerate(sents_offsets) if (sent_offset[0] >= titles_offsets_unique[-1][-1])])\n",
    "            titles_to_sents.append(tmp)\n",
    "\n",
    "        batch_concat_titles_ids.append(concat_titles_ids)\n",
    "        batch_title_positions.append(title_positions)\n",
    "        batch_sent_positions.append(sent_positions)\n",
    "        batch_titles_to_sents.append(titles_to_sents)\n",
    "\n",
    "    inputs.update({'context_start_id': ex_context_start_id})\n",
    "    inputs.update({'context_end_id': ex_context_end_id})\n",
    "    inputs.update({\"supp_title_labels\": batch_title_positions})\n",
    "    inputs.update({\"supp_sent_labels\": batch_sent_positions})\n",
    "    inputs.update({\"titles_to_sents\": batch_titles_to_sents})\n",
    "    inputs.update({\"concat_titles_ids\": batch_concat_titles_ids})\n",
    "\n",
    "    rearranged_inps = []\n",
    "    rearranged_masks = []\n",
    "    rearranged_offset_mapping = []\n",
    "\n",
    "    for ex_id in examples[\"id\"]:\n",
    "        ex_indices = np.where(np.array(raw_ids) == ex_id)[0].tolist()\n",
    "        rearranged_inps.append([inputs['input_ids'][i] for i in ex_indices])\n",
    "        rearranged_masks.append([inputs['attention_mask'][i] for i in ex_indices])\n",
    "        rearranged_offset_mapping.append([inputs['offset_mapping'][i] for i in ex_indices])\n",
    "\n",
    "    inputs.update({'input_ids': rearranged_inps})\n",
    "    inputs.update({'attention_mask': rearranged_masks})\n",
    "    inputs.update({'offset_mapping': rearranged_offset_mapping})\n",
    "\n",
    "    return inputs\n",
    "\n",
    "mem_val_dataset = val_examples.map(\n",
    "    preprocess_longformer_validation_examples,\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    remove_columns=val_examples.column_names,\n",
    ")\n",
    "mem_val_dataset.save_to_disk('../2wikimhqa_preprocessed_val_examples_512_multitask_stride20_one_doc_batched_without_zero_answer_pos_without_CoT_triplets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a267b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_venv",
   "language": "python",
   "name": "hf_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
