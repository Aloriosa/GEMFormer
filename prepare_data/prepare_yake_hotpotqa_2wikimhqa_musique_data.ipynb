{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de2f17cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle gemformer imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36a5ba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import RobertaTokenizerFast\n",
    "from datasets import load_from_disk\n",
    "from datasets.arrow_dataset import Dataset\n",
    "\n",
    "from gemformer.utils import add_qa_evidence_tokens, pad_and_drop_duplicates\n",
    "\n",
    "stride = 20\n",
    "max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ee7209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_mem(mem):\n",
    "    tokens = tokenizer(mem['mem'], max_length=202, truncation=True)\n",
    "    tokens.update({'mem': [kk[1:-1] for kk in tokens['input_ids']]})\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba63d79c",
   "metadata": {},
   "source": [
    "## MuSiQue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ef4ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "tokenizer = add_qa_evidence_tokens(tokenizer, tokens_to_add=['[para]'])\n",
    "\n",
    "max_num_answers = 1\n",
    "max_num_paragraphs = 20\n",
    "max_num_sentences = 152 # from train and val data\n",
    "eos = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)\n",
    "PARA_MARKER_token = tokenizer.convert_tokens_to_ids('[para]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a142abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.from_dict({'mem': torch.load(\"../yake_mem_strings_musique_train.pkl\")})\n",
    "mem_list = data.map(tokenize_mem,\n",
    "                    batched=True,\n",
    "                    remove_columns=data.column_names\n",
    "                   )\n",
    "train_dataset = torch.load('../musique_train_examples_allenai_style_with_para_seps.pkl')\n",
    "list_train_dataset = train_dataset[:]\n",
    "list_train_dataset.update({'mem_tokens': mem_list})\n",
    "mem_train_dataset = Dataset.from_dict(list_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcd3b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_roberta_training_examples_musique(train_examples):\n",
    "    questions = [q.strip() for q in train_examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        train_examples['context'],\n",
    "        max_length=max_length-len(train_examples['mem_tokens'][0]),\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    raw_ids = [train_examples['id'][kk] for kk in inputs['overflow_to_sample_mapping']]\n",
    "    # store start/end positions of context to filter part of sequence for uncertainty-based topk\n",
    "    inputs.update({\"id\": train_examples['id']})\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = train_examples['char_answer_offsets']\n",
    "    batch_question_type = [] # yes = 0, no = 1, span = 2\n",
    "    ex_context_start_id = []\n",
    "    ex_context_end_id = []\n",
    "    batch_start_positions_list = []\n",
    "    batch_end_positions_list = []\n",
    "    supp_paras = train_examples['supp_para_char_offsets']\n",
    "    batch_title_positions = []\n",
    "    batch_sent_positions = []\n",
    "    batch_titles_to_sents = []\n",
    "\n",
    "    for ex_id in train_examples[\"id\"]:\n",
    "        ex_indices = np.where(np.array(raw_ids) == ex_id)[0].tolist()\n",
    "        context_token_ids = [np.where(np.array(inputs['input_ids'][i]) == eos)[0] for i in ex_indices]\n",
    "        ex_context_start_id.append([elem[1] + 1 for elem in context_token_ids])\n",
    "        ex_context_end_id.append([elem[-1] for elem in context_token_ids])\n",
    "\n",
    "        start_positions_list = []\n",
    "        end_positions_list = []\n",
    "\n",
    "        supp_para_start_positions = []\n",
    "        supp_para_end_positions = []\n",
    "\n",
    "        for ex_sample in ex_indices:\n",
    "            offset_idx = ex_sample\n",
    "            offset = offset_mapping[offset_idx]\n",
    "            sample_idx = sample_map[offset_idx]\n",
    "            sequence_ids = inputs.sequence_ids(offset_idx)\n",
    "            # Find the start and end of the context\n",
    "            idx = 0\n",
    "            while sequence_ids[idx] != 1:\n",
    "                idx += 1\n",
    "            context_start = idx\n",
    "            while sequence_ids[idx] == 1:\n",
    "                idx += 1\n",
    "            context_end = idx - 1\n",
    "\n",
    "            start_positions = []\n",
    "            end_positions = []\n",
    "\n",
    "            for answer in [answers[sample_idx]]:\n",
    "                start_char = answer[0]\n",
    "                end_char = answer[1]\n",
    "\n",
    "                if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "                    continue\n",
    "                else:\n",
    "                    idx = context_start\n",
    "                    while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                        idx += 1\n",
    "                    start_positions.append(idx - 1)\n",
    "\n",
    "                    idx = context_end\n",
    "                    while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                        idx -= 1\n",
    "                    end_positions.append(idx + 1)\n",
    "\n",
    "        start_positions, end_positions = pad_and_drop_duplicates(start_positions, \n",
    "                                                                 end_positions, \n",
    "                                                                 max_num_answers)\n",
    "        start_positions_list.append(start_positions)\n",
    "        end_positions_list.append(end_positions)\n",
    "        \n",
    "        supp_para_start_positions.append([])\n",
    "        for supp_para_idx in range(len(supp_paras[sample_idx])):\n",
    "            supp_para = supp_paras[sample_idx][supp_para_idx]\n",
    "            supp_para_start_char = supp_para[0]\n",
    "            supp_para_end_char = supp_para[1]\n",
    "\n",
    "            if offset[context_start][0] > supp_para_start_char or offset[context_end][1] < supp_para_end_char:\n",
    "                continue\n",
    "            else:\n",
    "                idx1 = context_start\n",
    "                while idx1 <= context_end and offset[idx1][0] <= supp_para_start_char:\n",
    "                    idx1 += 1\n",
    "                supp_para_start_positions[-1].append(idx1 - 1)\n",
    "\n",
    "        title_positions = []\n",
    "        for i, supp in zip([inputs['input_ids'][ii] for ii in ex_indices], supp_para_start_positions):\n",
    "            title_positions.append([0 if j not in supp else 1 for j in np.where(np.array(i) == PARA_MARKER_token)[0]])\n",
    "\n",
    "            title_positions[-1] = pad_and_drop_duplicates(start_positions=title_positions[-1], \n",
    "                                                        max_num_answers=max_num_paragraphs)\n",
    "\n",
    "        batch_start_positions_list.append(start_positions_list)\n",
    "        batch_end_positions_list.append(end_positions_list)\n",
    "        batch_title_positions.append(title_positions)\n",
    "\n",
    "    inputs.update({\"start_positions\": batch_start_positions_list})\n",
    "    inputs.update({\"end_positions\": batch_end_positions_list})\n",
    "    inputs.update({'context_start_id': ex_context_start_id})\n",
    "    inputs.update({'context_end_id': ex_context_end_id})\n",
    "    inputs.update({\"supp_para_labels\": batch_title_positions})\n",
    "\n",
    "    rearranged_inps = []\n",
    "    rearranged_masks = []\n",
    "\n",
    "    for ex_id in train_examples[\"id\"]:\n",
    "        ex_indices = np.where(np.array(raw_ids) == ex_id)[0].tolist()\n",
    "        rearranged_inps.append([inputs['input_ids'][i] for i in ex_indices])\n",
    "        rearranged_masks.append([inputs['attention_mask'][i] for i in ex_indices])\n",
    "\n",
    "    inputs.update({'input_ids': rearranged_inps})\n",
    "    inputs.update({'attention_mask': rearranged_masks})\n",
    "\n",
    "    return inputs\n",
    "\n",
    "\n",
    "mem_train_dataset = mem_train_dataset.map(\n",
    "    preprocess_roberta_training_examples_musique,\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    remove_columns=mem_train_dataset.column_names,\n",
    ")\n",
    "\n",
    "tmp = mem_train_dataset[:]\n",
    "tmp.update({'mem_tokens': mem_list})\n",
    "mem_train_dataset = Dataset.from_dict(tmp)\n",
    "mem_train_dataset.save_to_disk('../musique_yake_mem_train_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e41e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.from_dict({'mem': torch.load(\"../yake_mem_strings_musique_val.pkl\")})\n",
    "mem_list = data.map(tokenize_mem,\n",
    "              batched=True,\n",
    "              remove_columns=data.column_names,\n",
    "             )\n",
    "val_dataset = torch.load('../musique_val_examples_allenai_style_with_para_seps_no_mem_seps.pkl')\n",
    "list_val_dataset = val_dataset[:]\n",
    "list_val_dataset.update({'mem_tokens': mem_list[:len(val_dataset)]})\n",
    "mem_val_dataset = Dataset.from_dict(list_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb35d55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_roberta_validation_examples_musique(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length-len(examples['mem_tokens'][0]),\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # store raw sample id to match subsamples related to the same big context document \n",
    "    raw_ids = [examples['id'][kk] for kk in inputs['overflow_to_sample_mapping']]\n",
    "    inputs.update({\"example_ids\": examples['id']})\n",
    "    offset_mapping = inputs[\"offset_mapping\"]\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    ex_context_start_id = []\n",
    "    ex_context_end_id = []\n",
    "    supp_paras = examples['supp_para_char_offsets']\n",
    "    batch_title_positions = []\n",
    "    batch_titles_to_sents = []\n",
    "    batch_concat_titles_ids = []\n",
    "\n",
    "    for ex_id in examples[\"id\"]:\n",
    "        ex_indices = np.where(np.array(raw_ids) == ex_id)[0].tolist()\n",
    "        context_token_ids = [np.where(np.array(inputs['input_ids'][i]) == eos)[0] for i in ex_indices]\n",
    "        ex_context_start_id.append([elem[1] + 1 for elem in context_token_ids])\n",
    "        ex_context_end_id.append([elem[-1] for elem in context_token_ids])\n",
    "\n",
    "        supp_para_start_positions = []\n",
    "\n",
    "        for ex_sample in ex_indices:\n",
    "\n",
    "            offset_idx = ex_sample\n",
    "            offset = offset_mapping[offset_idx]\n",
    "            #for offset_idx, offset in tqdm(enumerate(offset_mapping)):\n",
    "\n",
    "            sample_idx = sample_map[offset_idx]\n",
    "            sequence_ids = inputs.sequence_ids(offset_idx)\n",
    "\n",
    "            inputs[\"offset_mapping\"][offset_idx] = [\n",
    "                o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "            ]\n",
    "            # Find the start and end of the context\n",
    "            idx = 0\n",
    "            while sequence_ids[idx] != 1:\n",
    "                idx += 1\n",
    "            context_start = idx\n",
    "            while sequence_ids[idx] == 1:\n",
    "                idx += 1\n",
    "            context_end = idx - 1\n",
    "\n",
    "\n",
    "            supp_para_start_positions.append([])\n",
    "            for supp_para_idx in range(len(supp_paras[sample_idx])):\n",
    "                supp_para = supp_paras[sample_idx][supp_para_idx]\n",
    "                supp_para_start_char = supp_para[0]\n",
    "                supp_para_end_char = supp_para[1]\n",
    "\n",
    "                if offset[context_start][0] > supp_para_start_char or offset[context_end][1] < supp_para_end_char:\n",
    "                    continue\n",
    "                else:\n",
    "                    idx1 = context_start\n",
    "                    while idx1 <= context_end and offset[idx1][0] <= supp_para_start_char:\n",
    "                        idx1 += 1\n",
    "                    supp_para_start_positions[-1].append(idx1 - 1)\n",
    "\n",
    "        title_positions = []\n",
    "        for i, supp in zip([inputs['input_ids'][ii] for ii in ex_indices], supp_para_start_positions):\n",
    "            title_positions.append([0 if j not in supp else 1 for j in np.where(np.array(i) == PARA_MARKER_token)[0]])\n",
    "            title_positions[-1] = pad_and_drop_duplicates(start_positions=title_positions[-1], \n",
    "                                                          max_num_answers=max_num_paragraphs)\n",
    "\n",
    "        concat_input_ids = []\n",
    "        concat_offset_mapping = []\n",
    "        for ii in ex_indices:\n",
    "            concat_input_ids += inputs['input_ids'][ii]\n",
    "            concat_offset_mapping += inputs['offset_mapping'][ii]\n",
    "\n",
    "        titles = np.where(np.array(concat_input_ids) == PARA_MARKER_token)[0]\n",
    "        np_titles_concat_offset_mapping = np.array(concat_offset_mapping)[titles]\n",
    "        titles_offsets_unique = sorted(np.unique(np_titles_concat_offset_mapping.tolist(), axis=0).tolist())\n",
    "        concat_titles_ids = [titles_offsets_unique.index(list(i)) for i in np_titles_concat_offset_mapping]\n",
    "\n",
    "        batch_concat_titles_ids.append(concat_titles_ids)\n",
    "        batch_title_positions.append(title_positions)\n",
    "\n",
    "    inputs.update({'context_start_id': ex_context_start_id})\n",
    "    inputs.update({'context_end_id': ex_context_end_id})\n",
    "    inputs.update({\"supp_para_labels\": batch_title_positions})\n",
    "    inputs.update({\"concat_titles_ids\": batch_concat_titles_ids})\n",
    "\n",
    "    rearranged_inps = []\n",
    "    rearranged_masks = []\n",
    "    rearranged_offset_mapping = []\n",
    "\n",
    "    for ex_id in examples[\"id\"]:\n",
    "        ex_indices = np.where(np.array(raw_ids) == ex_id)[0].tolist()\n",
    "        rearranged_inps.append([inputs['input_ids'][i] for i in ex_indices])\n",
    "        rearranged_masks.append([inputs['attention_mask'][i] for i in ex_indices])\n",
    "        rearranged_offset_mapping.append([inputs['offset_mapping'][i] for i in ex_indices])\n",
    "\n",
    "    inputs.update({'input_ids': rearranged_inps})\n",
    "    inputs.update({'attention_mask': rearranged_masks})\n",
    "    inputs.update({'offset_mapping': rearranged_offset_mapping})\n",
    "\n",
    "    return inputs\n",
    "\n",
    "mem_val_dataset = mem_val_dataset.map(\n",
    "    preprocess_roberta_validation_examples_musique,\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    remove_columns=mem_val_dataset.column_names,\n",
    ")\n",
    "tmp = mem_val_dataset[:]\n",
    "tmp.update({'mem_tokens': mem_list[:len(mem_val_dataset)]})\n",
    "mem_val_dataset = Dataset.from_dict(tmp)\n",
    "mem_val_dataset.save_to_disk('../musique_yake_mem_val_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db14870",
   "metadata": {},
   "source": [
    "## HotpotQA and 2WikiMHQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a9f945",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "tokenizer = add_qa_evidence_tokens(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb8f11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.from_dict({'mem': torch.load(\"../yake_mem_strings_hotpotqa_train.pkl\")})\n",
    "mem_list = data.map(tokenize_mem,\n",
    "                    batched=True,\n",
    "                    remove_columns=data.column_names\n",
    "                   )\n",
    "list_train_dataset = torch.load('../hotpotqa_train_examples_with_special_seps.pkl')\n",
    "\n",
    "tmp = list_train_dataset[:]\n",
    "tmp.update({'mem_tokens': mem_list})\n",
    "mem_train_dataset = Dataset.from_dict(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59fe380",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_answers = 64\n",
    "max_num_paragraphs = 10 # by hotpotqa construction\n",
    "max_num_sentences = 150 # from train and val data\n",
    "eos = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)\n",
    "TITLE_END_token = tokenizer.convert_tokens_to_ids('</t>')\n",
    "TITLE_START_token = tokenizer.convert_tokens_to_ids('<t>')\n",
    "SENT_MARKER_END_token = tokenizer.convert_tokens_to_ids('[/sent]')\n",
    "    \n",
    "\n",
    "def preprocess_roberta_training_examples(train_examples):\n",
    "    questions = [q.strip() for q in train_examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        train_examples['context'],\n",
    "        max_length=max_length-len(train_examples['mem_tokens'][0]),\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    raw_ids = [train_examples['id'][kk] for kk in inputs['overflow_to_sample_mapping']]\n",
    "    # store start/end positions of context to filter part of sequence for uncertainty-based topk\n",
    "    inputs.update({\"id\": train_examples['id']})\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = train_examples['char_answer_offsets']\n",
    "    batch_question_type = [] # yes = 0, no = 1, span = 2\n",
    "    ex_context_start_id = []\n",
    "    ex_context_end_id = []\n",
    "    batch_start_positions_list = []\n",
    "    batch_end_positions_list = []\n",
    "    if train_examples.get('supp_sent_char_offsets', False):\n",
    "        supp_sents = train_examples['supp_sent_char_offsets']\n",
    "    if train_examples.get('supp_title_char_offsets', False):\n",
    "        supp_titles = train_examples['supp_title_char_offsets']\n",
    "    batch_title_positions = []\n",
    "    batch_sent_positions = []\n",
    "    batch_titles_to_sents = []\n",
    "\n",
    "    for ex_id in train_examples[\"id\"]:\n",
    "        ex_indices = np.where(np.array(raw_ids) == ex_id)[0].tolist()\n",
    "        context_token_ids = [np.where(np.array(inputs['input_ids'][i]) == eos)[0] for i in ex_indices]\n",
    "        ex_context_start_id.append([elem[1] + 1 for elem in context_token_ids])\n",
    "        ex_context_end_id.append([elem[-1] for elem in context_token_ids])\n",
    "\n",
    "        start_positions_list = []\n",
    "        end_positions_list = []\n",
    "        question_type = []\n",
    "        supp_sent_start_positions = []\n",
    "        supp_sent_end_positions = []\n",
    "        supp_title_start_positions = []\n",
    "        supp_title_end_positions = []\n",
    "\n",
    "        for ex_sample in ex_indices:\n",
    "            offset_idx = ex_sample\n",
    "            offset = offset_mapping[offset_idx]\n",
    "            sample_idx = sample_map[offset_idx]\n",
    "            sequence_ids = inputs.sequence_ids(offset_idx)\n",
    "            # Find the start and end of the context\n",
    "            idx = 0\n",
    "            while sequence_ids[idx] != 1:\n",
    "                idx += 1\n",
    "            context_start = idx\n",
    "            while sequence_ids[idx] == 1:\n",
    "                idx += 1\n",
    "            context_end = idx - 1\n",
    "\n",
    "            ques_type_start_char = answers[sample_idx][0][0]\n",
    "            ques_type_end_char = answers[sample_idx][0][1]\n",
    "            if ques_type_start_char == -1 and ques_type_end_char == -1:\n",
    "                question_type.append(0)\n",
    "            elif ques_type_start_char == -2 and ques_type_end_char == -2:\n",
    "                question_type.append(1)\n",
    "            else:\n",
    "                question_type.append(2)\n",
    "\n",
    "            start_positions = []\n",
    "            end_positions = []\n",
    "\n",
    "            for answer in answers[sample_idx]:\n",
    "                start_char = answer[0]\n",
    "                end_char = answer[1]\n",
    "\n",
    "                if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "                    continue\n",
    "                else:\n",
    "                    idx = context_start\n",
    "                    while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                        idx += 1\n",
    "                    start_positions.append(idx - 1)\n",
    "\n",
    "                    idx = context_end\n",
    "                    while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                        idx -= 1\n",
    "                    end_positions.append(idx + 1)\n",
    "\n",
    "        start_positions, end_positions = pad_and_drop_duplicates(start_positions, \n",
    "                                                                 end_positions, \n",
    "                                                                 max_num_answers)\n",
    "        start_positions_list.append(start_positions)\n",
    "        end_positions_list.append(end_positions)\n",
    "\n",
    "        supp_title_start_positions.append([])\n",
    "        supp_title_end_positions.append([])\n",
    "\n",
    "        for supp_title_idx in range(len(supp_titles[sample_idx])):\n",
    "            supp_title = supp_titles[sample_idx][supp_title_idx]\n",
    "            supp_title_start_char = supp_title[0]\n",
    "            supp_title_end_char = supp_title[1]\n",
    "\n",
    "            if offset[context_start][0] > supp_title_start_char or offset[context_end][1] < supp_title_end_char:\n",
    "                continue\n",
    "            else:\n",
    "                idx1 = context_start\n",
    "                while idx1 <= context_end and offset[idx1][0] <= supp_title_start_char:\n",
    "                    idx1 += 1\n",
    "                supp_title_start_positions[-1].append(idx1 - 1)\n",
    "\n",
    "                idx1 = context_end\n",
    "                while idx1 >= context_start and offset[idx1][1] >= supp_title_end_char:\n",
    "                    idx1 -= 1\n",
    "                supp_title_end_positions[-1].append(idx1 + 1)\n",
    "\n",
    "        supp_sent_start_positions.append([])\n",
    "        supp_sent_end_positions.append([])\n",
    "        for supp_sent_idx in range(len(supp_sents[sample_idx])):\n",
    "            supp_sent = supp_sents[sample_idx][supp_sent_idx]\n",
    "            supp_sent_start_char = supp_sent[0]\n",
    "            supp_sent_end_char = supp_sent[1]\n",
    "\n",
    "            if offset[context_start][0] > supp_sent_start_char or offset[context_end][1] < supp_sent_end_char:\n",
    "                continue\n",
    "            else:\n",
    "                idx2 = context_start\n",
    "                while idx2 <= context_end and offset[idx2][0] <= supp_sent_start_char:\n",
    "                    idx2 += 1\n",
    "                supp_sent_start_positions[-1].append(idx2 - 1)\n",
    "\n",
    "                idx2 = context_end\n",
    "                while idx2 >= context_start and offset[idx2][1] >= supp_sent_end_char:\n",
    "                    idx2 -= 1\n",
    "                supp_sent_end_positions[-1].append(idx2 + 1)\n",
    "\n",
    "        title_positions = []\n",
    "        for i, supp in zip([inputs['input_ids'][ii] for ii in ex_indices], supp_title_start_positions):\n",
    "            title_positions.append([0 if j not in supp else 1 for j in np.where(np.array(i) == TITLE_START_token)[0]])\n",
    "            title_positions[-1] = pad_and_drop_duplicates(start_positions=title_positions[-1], \n",
    "                                                          max_num_answers=max_num_paragraphs)\n",
    "\n",
    "\n",
    "        sent_positions = []\n",
    "        for i, supp in zip([inputs['input_ids'][ii] for ii in ex_indices], supp_sent_end_positions):\n",
    "            sent_positions.append([0 if j not in supp else 1 for j in np.where(np.array(i) == SENT_MARKER_END_token)[0]])\n",
    "            sent_positions[-1] = pad_and_drop_duplicates(start_positions=sent_positions[-1], \n",
    "                                                         max_num_answers=max_num_sentences)\n",
    "\n",
    "\n",
    "\n",
    "        batch_question_type.append(question_type)\n",
    "        batch_start_positions_list.append(start_positions_list)\n",
    "        batch_end_positions_list.append(end_positions_list)\n",
    "        batch_title_positions.append(title_positions)\n",
    "        batch_sent_positions.append(sent_positions)\n",
    "\n",
    "    inputs.update({\"start_positions\": batch_start_positions_list})\n",
    "    inputs.update({\"end_positions\": batch_end_positions_list})\n",
    "    inputs.update({'question_type': batch_question_type})\n",
    "    inputs.update({'context_start_id': ex_context_start_id})\n",
    "    inputs.update({'context_end_id': ex_context_end_id})\n",
    "    inputs.update({\"supp_title_labels\": batch_title_positions})\n",
    "    inputs.update({\"supp_sent_labels\": batch_sent_positions})\n",
    "\n",
    "    rearranged_inps = []\n",
    "    rearranged_masks = []\n",
    "\n",
    "    for ex_id in train_examples[\"id\"]:\n",
    "        ex_indices = np.where(np.array(raw_ids) == ex_id)[0].tolist()\n",
    "        rearranged_inps.append([inputs['input_ids'][i] for i in ex_indices])\n",
    "        rearranged_masks.append([inputs['attention_mask'][i] for i in ex_indices])\n",
    "\n",
    "    inputs.update({'input_ids': rearranged_inps})\n",
    "    inputs.update({'attention_mask': rearranged_masks})\n",
    "\n",
    "    return inputs\n",
    "\n",
    "\n",
    "mem_train_dataset = mem_train_dataset.map(\n",
    "    preprocess_roberta_training_examples,\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    remove_columns=mem_train_dataset.column_names,\n",
    ")\n",
    "\n",
    "tmp = mem_train_dataset[:]\n",
    "tmp.update({'mem_tokens': mem_list})\n",
    "mem_train_dataset = Dataset.from_dict(tmp)\n",
    "mem_train_dataset.save_to_disk('../hotpotqa_yake_mem_train_dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dc30b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = Dataset.from_dict({'mem': torch.load(\"../yake_mem_strings_hotpotqa_val.pkl\")})\n",
    "mem_list = data.map(tokenize_mem,\n",
    "                    batched=True,\n",
    "                    remove_columns=data.column_names\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a0b08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_val_dataset = torch.load('../hotpotqa_val_examples_with_special_seps.pkl')\n",
    "\n",
    "tmp = list_val_dataset[:]\n",
    "tmp.update({'mem_tokens': mem_list[:len(list_val_dataset)]})\n",
    "mem_val_dataset = Dataset.from_dict(tmp)\n",
    "\n",
    "\n",
    "def preprocess_roberta_validation_examples(examples):\n",
    "\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length-len(examples['mem_tokens'][0]),\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    raw_ids = [examples['id'][kk] for kk in inputs['overflow_to_sample_mapping']]\n",
    "    inputs.update({\"example_ids\": examples['id']})\n",
    "    offset_mapping = inputs[\"offset_mapping\"]\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    ex_context_start_id = []\n",
    "    ex_context_end_id = []\n",
    "    supp_sents = examples['supp_sent_char_offsets']\n",
    "    supp_titles = examples['supp_title_char_offsets']\n",
    "    batch_title_positions = []\n",
    "    batch_sent_positions = []\n",
    "    batch_titles_to_sents = []\n",
    "    batch_concat_titles_ids = []\n",
    "\n",
    "    for ex_id in examples[\"id\"]:\n",
    "        ex_indices = np.where(np.array(raw_ids) == ex_id)[0].tolist()\n",
    "        context_token_ids = [np.where(np.array(inputs['input_ids'][i]) == eos)[0] for i in ex_indices]\n",
    "        ex_context_start_id.append([elem[1] + 1 for elem in context_token_ids])\n",
    "        ex_context_end_id.append([elem[-1] for elem in context_token_ids])\n",
    "\n",
    "        supp_sent_start_positions = []\n",
    "        supp_sent_end_positions = []\n",
    "        supp_title_start_positions = []\n",
    "        supp_title_end_positions = []\n",
    "\n",
    "        for ex_sample in ex_indices:\n",
    "\n",
    "            offset_idx = ex_sample\n",
    "            offset = offset_mapping[offset_idx]\n",
    "\n",
    "            sample_idx = sample_map[offset_idx]\n",
    "            sequence_ids = inputs.sequence_ids(offset_idx)\n",
    "\n",
    "            inputs[\"offset_mapping\"][offset_idx] = [\n",
    "                o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "            ]\n",
    "            # Find the start and end of the context\n",
    "            idx = 0\n",
    "            while sequence_ids[idx] != 1:\n",
    "                idx += 1\n",
    "            context_start = idx\n",
    "            while sequence_ids[idx] == 1:\n",
    "                idx += 1\n",
    "            context_end = idx - 1\n",
    "\n",
    "\n",
    "            supp_title_start_positions.append([])\n",
    "            supp_title_end_positions.append([])\n",
    "            for supp_title_idx in range(len(supp_titles[sample_idx])):\n",
    "                supp_title = supp_titles[sample_idx][supp_title_idx]\n",
    "                supp_title_start_char = supp_title[0]\n",
    "                supp_title_end_char = supp_title[1]\n",
    "\n",
    "                if offset[context_start][0] > supp_title_start_char or offset[context_end][1] < supp_title_end_char:\n",
    "                    continue\n",
    "                else:\n",
    "                    idx1 = context_start\n",
    "                    while idx1 <= context_end and offset[idx1][0] <= supp_title_start_char:\n",
    "                        idx1 += 1\n",
    "                    supp_title_start_positions[-1].append(idx1 - 1)\n",
    "\n",
    "                    idx1 = context_end\n",
    "                    while idx1 >= context_start and offset[idx1][1] >= supp_title_end_char:\n",
    "                        idx1 -= 1\n",
    "                    supp_title_end_positions[-1].append(idx1 + 1)\n",
    "\n",
    "            supp_sent_start_positions.append([])\n",
    "            supp_sent_end_positions.append([])\n",
    "            for supp_sent_idx in range(len(supp_sents[sample_idx])):\n",
    "                supp_sent = supp_sents[sample_idx][supp_sent_idx]\n",
    "                supp_sent_start_char = supp_sent[0]\n",
    "                supp_sent_end_char = supp_sent[1]\n",
    "\n",
    "                if offset[context_start][0] > supp_sent_start_char or offset[context_end][1] < supp_sent_end_char:\n",
    "                    continue\n",
    "                else:\n",
    "                    idx2 = context_start\n",
    "                    while idx2 <= context_end and offset[idx2][0] <= supp_sent_start_char:\n",
    "                        idx2 += 1\n",
    "                    supp_sent_start_positions[-1].append(idx2 - 1)\n",
    "\n",
    "                    idx2 = context_end\n",
    "                    while idx2 >= context_start and offset[idx2][1] >= supp_sent_end_char:\n",
    "                        idx2 -= 1\n",
    "                    supp_sent_end_positions[-1].append(idx2 + 1)\n",
    "\n",
    "\n",
    "\n",
    "        title_positions = []\n",
    "        for i, supp in zip([inputs['input_ids'][ii] for ii in ex_indices], supp_title_start_positions):\n",
    "            title_positions.append([0 if j not in supp else 1 for j in np.where(np.array(i) == TITLE_START_token)[0]])\n",
    "            title_positions[-1] = pad_and_drop_duplicates(start_positions=title_positions[-1], \n",
    "                                                          max_num_answers=max_num_paragraphs)\n",
    "\n",
    "\n",
    "        sent_positions = []\n",
    "        for i, supp in zip([inputs['input_ids'][ii] for ii in ex_indices], supp_sent_end_positions):\n",
    "            sent_positions.append([0 if j not in supp else 1 for j in np.where(np.array(i) == SENT_MARKER_END_token)[0]])\n",
    "            sent_positions[-1] = pad_and_drop_duplicates(start_positions=sent_positions[-1], \n",
    "                                                         max_num_answers=max_num_sentences)\n",
    "\n",
    "\n",
    "        concat_input_ids = []\n",
    "        concat_offset_mapping = []\n",
    "        for ii in ex_indices:\n",
    "            concat_input_ids += inputs['input_ids'][ii]\n",
    "            concat_offset_mapping += inputs['offset_mapping'][ii]\n",
    "\n",
    "        titles = np.where(np.array(concat_input_ids) == TITLE_START_token)[0]\n",
    "        np_titles_concat_offset_mapping = np.array(concat_offset_mapping)[titles]\n",
    "        titles_offsets_unique = sorted(np.unique(np_titles_concat_offset_mapping.tolist(), axis=0).tolist())\n",
    "        concat_titles_ids = [titles_offsets_unique.index(list(i)) for i in np_titles_concat_offset_mapping]\n",
    "\n",
    "\n",
    "        global_sents = np.where(np.array(concat_input_ids) == SENT_MARKER_END_token)[0]#.tolist()\n",
    "        global_sents_offsets_unique = []\n",
    "        for i in np.array(concat_offset_mapping)[global_sents]:\n",
    "            if i not in global_sents_offsets_unique:\n",
    "                global_sents_offsets_unique.append(i)\n",
    "\n",
    "\n",
    "\n",
    "        titles_to_sents = []\n",
    "\n",
    "        for i, offsets_list in zip([inputs['input_ids'][ii] for ii in ex_indices], \n",
    "                                   [inputs['offset_mapping'][ii] for ii in ex_indices]):\n",
    "            sents = np.where(np.array(i) == SENT_MARKER_END_token)[0].tolist()\n",
    "            sents_offsets = np.array(offsets_list)[sents]\n",
    "\n",
    "            tmp = []\n",
    "            if len(titles_offsets_unique) > 1:\n",
    "                #local chunk sent_id, global doc sent_id\n",
    "                tmp = [[(sent_id, global_sents_offsets_unique.index(sent_offset)) for sent_id, sent_offset in enumerate(sents_offsets) if ((sent_offset[0] >= title_offset[-1]) and (sent_offset[-1] <= titles_offsets_unique[i+1][0])) ] for i, title_offset in enumerate(titles_offsets_unique[:-1])]\n",
    "\n",
    "            if len(titles_offsets_unique) > 0:\n",
    "                tmp.append([(sent_id, global_sents_offsets_unique.index(sent_offset)) for sent_id, sent_offset in enumerate(sents_offsets) if (sent_offset[0] >= titles_offsets_unique[-1][-1])])\n",
    "\n",
    "            titles_to_sents.append(tmp)\n",
    "\n",
    "        batch_concat_titles_ids.append(concat_titles_ids)\n",
    "        batch_title_positions.append(title_positions)\n",
    "        batch_sent_positions.append(sent_positions)\n",
    "        batch_titles_to_sents.append(titles_to_sents)\n",
    "\n",
    "    inputs.update({'context_start_id': ex_context_start_id})\n",
    "    inputs.update({'context_end_id': ex_context_end_id})\n",
    "    inputs.update({\"supp_title_labels\": batch_title_positions})\n",
    "    inputs.update({\"supp_sent_labels\": batch_sent_positions})\n",
    "    inputs.update({\"titles_to_sents\": batch_titles_to_sents})\n",
    "    inputs.update({\"concat_titles_ids\": batch_concat_titles_ids})\n",
    "\n",
    "    rearranged_inps = []\n",
    "    rearranged_masks = []\n",
    "    rearranged_offset_mapping = []\n",
    "\n",
    "    for ex_id in examples[\"id\"]:\n",
    "        ex_indices = np.where(np.array(raw_ids) == ex_id)[0].tolist()\n",
    "        rearranged_inps.append([inputs['input_ids'][i] for i in ex_indices])\n",
    "        rearranged_masks.append([inputs['attention_mask'][i] for i in ex_indices])\n",
    "        rearranged_offset_mapping.append([inputs['offset_mapping'][i] for i in ex_indices])\n",
    "\n",
    "    inputs.update({'input_ids': rearranged_inps})\n",
    "    inputs.update({'attention_mask': rearranged_masks})\n",
    "    inputs.update({'offset_mapping': rearranged_offset_mapping})\n",
    "\n",
    "    return inputs\n",
    "\n",
    "mem_val_dataset = mem_val_dataset.map(\n",
    "    preprocess_roberta_validation_examples,\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    remove_columns=mem_val_dataset.column_names,\n",
    ")\n",
    "tmp = mem_val_dataset[:]\n",
    "tmp.update({'mem_tokens': mem_list[:len(mem_val_dataset)]})\n",
    "mem_val_dataset = Dataset.from_dict(tmp)\n",
    "mem_val_dataset.save_to_disk('../hotpotqa_yake_mem_val_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222cceb1",
   "metadata": {},
   "source": [
    "### 2wikiMHQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1020679d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7554f996972348cca14554c5db70eceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/167454 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "tokenizer = add_qa_evidence_tokens(tokenizer)\n",
    "\n",
    "data = Dataset.from_dict({'mem': torch.load(\"../yake_mem_strings_2wikimhqa_train.pkl\")})\n",
    "mem_list = data.map(tokenize_mem,\n",
    "                    batched=True,\n",
    "                    remove_columns=data.column_names\n",
    "                   )\n",
    "\n",
    "list_train_dataset = torch.load('../2wikimhqa_train_examples_with_special_seps.pkl')\n",
    "\n",
    "tmp = list_train_dataset[:]\n",
    "tmp.update({'mem_tokens': mem_list})\n",
    "mem_train_dataset = Dataset.from_dict(tmp)\n",
    "\n",
    "max_num_answers = 64\n",
    "max_num_paragraphs = 10 # by hotpotqa construction\n",
    "max_num_sentences = 210 # checked from train and val data\n",
    "eos = tokenizer.convert_tokens_to_ids(tokenizer.eos_token) #'</s>'\n",
    "TITLE_END_token = tokenizer.convert_tokens_to_ids('</t>')   # indicating the end of the title of a paragraph\n",
    "\n",
    "mem_train_dataset = mem_train_dataset.map(\n",
    "    preprocess_roberta_training_examples,\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    remove_columns=mem_train_dataset.column_names,\n",
    ")\n",
    "\n",
    "tmp = mem_train_dataset[:]\n",
    "tmp.update({'mem_tokens': mem_list})\n",
    "mem_train_dataset = Dataset.from_dict(tmp)\n",
    "mem_train_dataset.save_to_disk('../2wikimhqa_yake_mem_train_dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff86dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.from_dict({'mem': torch.load(\"../yake_mem_strings_2wikimhqa_val.pkl\")})\n",
    "mem_list = data.map(tokenize_mem,\n",
    "                    batched=True,\n",
    "                    remove_columns=data.column_names\n",
    "                   )\n",
    "list_val_dataset = torch.load('../2wikimhqa_val_examples_with_special_seps_with_mem_seps.pkl')\n",
    "\n",
    "tmp = list_val_dataset[:]\n",
    "tmp.update({'mem_tokens': mem_list[:len(list_val_dataset)]})\n",
    "mem_val_dataset = Dataset.from_dict(tmp)\n",
    "\n",
    "mem_val_dataset = mem_val_dataset.map(\n",
    "    preprocess_roberta_validation_examples,\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    remove_columns=mem_val_dataset.column_names,\n",
    ")\n",
    "tmp = mem_val_dataset[:]\n",
    "tmp.update({'mem_tokens': mem_list[:len(mem_val_dataset)]})\n",
    "mem_val_dataset = Dataset.from_dict(tmp)\n",
    "mem_val_dataset.save_to_disk('../2wikimhqa_yake_mem_val_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02276102",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_venv",
   "language": "python",
   "name": "hf_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
