{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe6d4c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19938it [00:01, 12782.70it/s]\n",
      "2417it [00:00, 9188.11it/s]\n",
      "2459it [00:00, 15139.85it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# download data from https://github.com/StonyBrookNLP/musique\n",
    "\n",
    "with open('./musique_data/musique_ans_v1.0_train.jsonl', \"r\") as file:\n",
    "    train = [json.loads(line.strip()) for line in tqdm(file) if line.strip()]\n",
    "with open('./musique_data/musique_ans_v1.0_dev.jsonl', \"r\") as file:\n",
    "    dev = [json.loads(line.strip()) for line in tqdm(file) if line.strip()]\n",
    "with open('./musique_data/musique_ans_v1.0_test.jsonl', \"r\") as file:\n",
    "    test = [json.loads(line.strip()) for line in tqdm(file) if line.strip()]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19fb4984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "\n",
    "def to_hotpotqa_format(train, test=False):\n",
    "    for ex in tqdm(train):\n",
    "        cntxt_title = []\n",
    "        cntxt_sents = []\n",
    "        supp_title = []\n",
    "        supp_sent_id = []\n",
    "\n",
    "        for para in ex['paragraphs']:\n",
    "            cntxt_title.append(para['title'].encode('utf-8', 'replace').decode('utf-8')) \n",
    "            para_sents = [((' ' if sent_i > 0 else '') + sent).encode('utf-8', 'replace').decode('utf-8') for sent_i, sent in enumerate(sent_tokenize(para['paragraph_text']))]\n",
    "            cntxt_sents.append(para_sents)\n",
    "            if not test:\n",
    "                if para['is_supporting'] == True:\n",
    "                    supp_title += [para['title'].encode('utf-8', 'replace').decode('utf-8')]\n",
    "                    \n",
    "        ex.update({'supporting_facts': {'title': supp_title,\n",
    "                                        'sent_id': []},\n",
    "                   'context': {'title': cntxt_title,\n",
    "                               'sentences': cntxt_sents}\n",
    "            })\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8602471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 19938/19938 [00:42<00:00, 470.65it/s]\n",
      "100%|██████████████████████████████████████| 2417/2417 [00:04<00:00, 487.04it/s]\n"
     ]
    }
   ],
   "source": [
    "prepro_train = to_hotpotqa_format(train)\n",
    "prepro_dev = to_hotpotqa_format(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cd93c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19938, 2417)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prepro_train), len(prepro_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07d03538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c360bb303e304f8882ee5a121801b99c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64f0419a0a8c40b99df411019231f46d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2417 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(152, 125)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([sum([len(j) for j in ex['context']['sentences']]) for ex in tqdm(prepro_train)]), \\\n",
    "max([sum([len(j) for j in ex['context']['sentences']]) for ex in tqdm(prepro_dev)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b2dde9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(ex['context']['title']) for ex in prepro_train]), \\\n",
    "max([len(ex['context']['title']) for ex in prepro_dev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5114a707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'paragraphs', 'question', 'question_decomposition', 'answer', 'answer_aliases', 'answerable', 'supporting_facts', 'context'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepro_train[-1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c86ea4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 15:30:25.549929: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-29 15:30:26.639300: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from prepro_char_based_targets_musique import process_file\n",
    "from datasets.arrow_dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a70c368b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done   1 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=12)]: Done   8 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=12)]: Done  17 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=12)]: Batch computation too fast (0.1632s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=12)]: Done  37 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=12)]: Batch computation too fast (0.0085s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=12)]: Done  58 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=12)]: Done  84 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=12)]: Batch computation too fast (0.0112s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=12)]: Done 134 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=12)]: Batch computation too fast (0.0175s.) Setting batch_size=16.\n",
      "[Parallel(n_jobs=12)]: Done 199 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=12)]: Done 291 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=12)]: Batch computation too fast (0.0396s.) Setting batch_size=32.\n",
      "[Parallel(n_jobs=12)]: Done 475 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=12)]: Done 717 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=12)]: Batch computation too fast (0.0597s.) Setting batch_size=64.\n",
      "[Parallel(n_jobs=12)]: Done 1215 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=12)]: Batch computation too fast (0.1034s.) Setting batch_size=128.\n",
      "[Parallel(n_jobs=12)]: Done 2048 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=12)]: Batch computation too fast (0.1966s.) Setting batch_size=256.\n",
      "[Parallel(n_jobs=12)]: Done 3712 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=12)]: Done 6656 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=12)]: Done 12544 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=12)]: Done 17944 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=12)]: Done 18732 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=12)]: Done 19032 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=12)]: Done 19356 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=12)]: Done 19938 out of 19938 | elapsed:    5.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19938 questions in total\n"
     ]
    }
   ],
   "source": [
    "train_examples = process_file(prepro_train, with_special_seps=True, with_mem_seps=False)\n",
    "train_examples = Dataset.from_dict({feature: [train_examples[i][feature] for i in range(len(train_examples))] for feature in train_examples[0]})\n",
    "torch.save(train_examples, '../musique_train_examples_allenai_style_with_para_seps.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "777de7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Batch computation too fast (0.0055s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=12)]: Done   8 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Batch computation too fast (0.0130s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=12)]: Done  28 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done  50 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done  72 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Batch computation too fast (0.0160s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=12)]: Done 124 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Batch computation too fast (0.0215s.) Setting batch_size=16.\n",
      "[Parallel(n_jobs=12)]: Done 184 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=12)]: Done 304 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=12)]: Batch computation too fast (0.0313s.) Setting batch_size=32.\n",
      "[Parallel(n_jobs=12)]: Done 488 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=12)]: Done 776 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=12)]: Batch computation too fast (0.0564s.) Setting batch_size=64.\n",
      "[Parallel(n_jobs=12)]: Done 1320 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=12)]: Batch computation too fast (0.0973s.) Setting batch_size=128.\n",
      "[Parallel(n_jobs=12)]: Done 1903 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=12)]: Done 2048 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=12)]: Done 2069 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=12)]: Done 2216 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=12)]: Done 2239 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=12)]: Done 2262 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=12)]: Done 2417 out of 2417 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2417 questions in total\n"
     ]
    }
   ],
   "source": [
    "val_examples = process_file(prepro_dev, with_special_seps=True, with_mem_seps=False)\n",
    "val_examples = Dataset.from_dict({feature: [val_examples[i][feature] for i in range(len(val_examples))] for feature in val_examples[0]})\n",
    "torch.save(val_examples, '../musique_val_examples_allenai_style_with_para_seps.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76ad3f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle gemformer imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2a7c84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 16:13:56.595548: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-29 16:13:57.663361: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial vocab len = 50265\n",
      "We have added 1 tokens\n",
      "final vocab len = 50266\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import RobertaTokenizerFast, default_data_collator\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets.arrow_dataset import Dataset\n",
    "from datasets import load_from_disk\n",
    "from gemformer.utils import add_qa_evidence_tokens, pad_and_drop_duplicates, ROBERTA_BASE_SPECIAL_TOKENS\n",
    "\n",
    "tokenizer_name = 'roberta-base'\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_name)\n",
    "tokenizer = add_qa_evidence_tokens(tokenizer, tokens_to_add=['[para]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab96f913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945c35fefe744648a08ab5ce8734463a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19938 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_examples = torch.load('../musique_train_examples_allenai_style_with_para_seps.pkl')\n",
    "\n",
    "stride = 20\n",
    "max_num_answers = 1\n",
    "max_num_paragraphs = 20 # by dataset construction\n",
    "max_num_sentences = 152 # from train and val data\n",
    "eos = tokenizer.convert_tokens_to_ids(tokenizer.eos_token) #'</s>'\n",
    "PARA_MARKER_token = tokenizer.convert_tokens_to_ids('[para]') #para start\n",
    "SENT_MARKER_END_token = tokenizer.convert_tokens_to_ids('[/sent]')\n",
    "MAX_SEQ_LEN = 512\n",
    "\n",
    "def preprocess_roberta_long_training_examples(train_examples):\n",
    "    questions = [q.strip() for q in train_examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        train_examples['context'],\n",
    "        max_length=MAX_SEQ_LEN,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    raw_ids = [train_examples['id'][kk] for kk in inputs['overflow_to_sample_mapping']]\n",
    "    inputs.update({\"id\": train_examples['id']})\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = train_examples['char_answer_offsets']\n",
    "    ex_context_start_id = []\n",
    "    ex_context_end_id = []\n",
    "    batch_start_positions_list = []\n",
    "    batch_end_positions_list = []\n",
    "    supp_paras = train_examples['supp_para_char_offsets']\n",
    "    batch_title_positions = []\n",
    "    batch_sent_positions = []\n",
    "    batch_titles_to_sents = []\n",
    "\n",
    "    for ex_id in train_examples[\"id\"]:\n",
    "        ex_indices = np.where(np.array(raw_ids) == ex_id)[0].tolist()\n",
    "        context_token_ids = [np.where(np.array(inputs['input_ids'][i]) == eos)[0] for i in ex_indices]\n",
    "        ex_context_start_id.append([elem[1] + 1 for elem in context_token_ids])\n",
    "        ex_context_end_id.append([elem[-1] for elem in context_token_ids])\n",
    "\n",
    "        start_positions_list = []\n",
    "        end_positions_list = []\n",
    "      \n",
    "        supp_para_start_positions = []\n",
    "        supp_para_end_positions = []\n",
    "\n",
    "        for ex_sample in ex_indices:\n",
    "            offset_idx = ex_sample\n",
    "            offset = offset_mapping[offset_idx]\n",
    "            sample_idx = sample_map[offset_idx]\n",
    "            sequence_ids = inputs.sequence_ids(offset_idx)\n",
    "            # Find the start and end of the context\n",
    "            idx = 0\n",
    "            while sequence_ids[idx] != 1:\n",
    "                idx += 1\n",
    "            context_start = idx\n",
    "            while sequence_ids[idx] == 1:\n",
    "                idx += 1\n",
    "            context_end = idx - 1\n",
    "\n",
    "            start_positions = []\n",
    "            end_positions = []\n",
    "\n",
    "            for answer in [answers[sample_idx]]:\n",
    "                start_char = answer[0]\n",
    "                end_char = answer[1]\n",
    "\n",
    "                if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "                    continue\n",
    "                else:\n",
    "                    idx = context_start\n",
    "                    while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                        idx += 1\n",
    "                    start_positions.append(idx - 1)\n",
    "\n",
    "                    idx = context_end\n",
    "                    while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                        idx -= 1\n",
    "                    end_positions.append(idx + 1)\n",
    "\n",
    "            start_positions, end_positions = pad_and_drop_duplicates(start_positions, end_positions, max_num_answers)\n",
    "            start_positions_list.append(start_positions)\n",
    "            end_positions_list.append(end_positions)\n",
    "\n",
    "            supp_para_start_positions.append([])\n",
    "            for supp_para_idx in range(len(supp_paras[sample_idx])):\n",
    "                supp_para = supp_paras[sample_idx][supp_para_idx]\n",
    "                supp_para_start_char = supp_para[0]\n",
    "                supp_para_end_char = supp_para[1]\n",
    "\n",
    "                if offset[context_start][0] > supp_para_start_char or offset[context_end][1] < supp_para_end_char:\n",
    "                    continue\n",
    "                else:\n",
    "                    idx1 = context_start\n",
    "                    while idx1 <= context_end and offset[idx1][0] <= supp_para_start_char:\n",
    "                        idx1 += 1\n",
    "                    supp_para_start_positions[-1].append(idx1 - 1)\n",
    "\n",
    "        \n",
    "        title_positions = []\n",
    "        for i, supp in zip([inputs['input_ids'][ii] for ii in ex_indices], supp_para_start_positions):\n",
    "            title_positions.append([0 if j not in supp else 1 for j in np.where(np.array(i) == PARA_MARKER_token)[0]])\n",
    "            title_positions[-1] = pad_and_drop_duplicates(start_positions=title_positions[-1], \n",
    "                                                          max_num_answers=max_num_paragraphs)\n",
    "\n",
    "        batch_start_positions_list.append(start_positions_list)\n",
    "        batch_end_positions_list.append(end_positions_list)\n",
    "        batch_title_positions.append(title_positions)\n",
    "      \n",
    "    inputs.update({\"start_positions\": batch_start_positions_list})\n",
    "    inputs.update({\"end_positions\": batch_end_positions_list})\n",
    "    inputs.update({'context_start_id': ex_context_start_id})\n",
    "    inputs.update({'context_end_id': ex_context_end_id})\n",
    "    inputs.update({\"supp_para_labels\": batch_title_positions})\n",
    "    \n",
    "    rearranged_inps = []\n",
    "    rearranged_masks = []\n",
    "\n",
    "    for ex_id in train_examples[\"id\"]:\n",
    "        ex_indices = np.where(np.array(raw_ids) == ex_id)[0].tolist()\n",
    "        rearranged_inps.append([inputs['input_ids'][i] for i in ex_indices])\n",
    "        rearranged_masks.append([inputs['attention_mask'][i] for i in ex_indices])\n",
    "\n",
    "    inputs.update({'input_ids': rearranged_inps})\n",
    "    inputs.update({'attention_mask': rearranged_masks})\n",
    "\n",
    "\n",
    "    return inputs\n",
    "\n",
    "\n",
    "mem_train_dataset = train_examples.map(\n",
    "    preprocess_roberta_long_training_examples,\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    remove_columns=train_examples.column_names,\n",
    ")\n",
    "\n",
    "mem_train_dataset.save_to_disk('../musique_preprocessed_train_examples_512_allanai_style_multitask_stride20_one_doc_batched_without_zero_answer_pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "962c326d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3892a22ec6434496ed04ae1f0e0848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2417 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_192902/1701383704.py:119: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np_titles_concat_offset_mapping = np.array(concat_offset_mapping)[titles]#list of tuples\n"
     ]
    }
   ],
   "source": [
    "val_examples = torch.load('../musique_val_examples_allenai_style_with_para_seps.pkl')\n",
    "\n",
    "def preprocess_longformer_validation_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_SEQ_LEN,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    raw_ids = [examples['id'][kk] for kk in inputs['overflow_to_sample_mapping']]\n",
    "    inputs.update({\"example_ids\": examples['id']})\n",
    "    offset_mapping = inputs[\"offset_mapping\"]\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    ex_context_start_id = []\n",
    "    ex_context_end_id = []\n",
    "    supp_paras = examples['supp_para_char_offsets']\n",
    "    batch_title_positions = []\n",
    "    batch_titles_to_sents = []\n",
    "    batch_concat_titles_ids = []\n",
    "\n",
    "    for ex_id in examples[\"id\"]:\n",
    "        ex_indices = np.where(np.array(raw_ids) == ex_id)[0].tolist()\n",
    "        context_token_ids = [np.where(np.array(inputs['input_ids'][i]) == eos)[0] for i in ex_indices]\n",
    "        ex_context_start_id.append([elem[1] + 1 for elem in context_token_ids])\n",
    "        ex_context_end_id.append([elem[-1] for elem in context_token_ids])\n",
    "\n",
    "        supp_para_start_positions = []\n",
    "      \n",
    "        for ex_sample in ex_indices:\n",
    "            offset_idx = ex_sample\n",
    "            offset = offset_mapping[offset_idx]\n",
    "            sample_idx = sample_map[offset_idx]\n",
    "            sequence_ids = inputs.sequence_ids(offset_idx)\n",
    "\n",
    "            inputs[\"offset_mapping\"][offset_idx] = [\n",
    "                o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "            ]\n",
    "            # Find the start and end of the context\n",
    "            idx = 0\n",
    "            while sequence_ids[idx] != 1:\n",
    "                idx += 1\n",
    "            context_start = idx\n",
    "            while sequence_ids[idx] == 1:\n",
    "                idx += 1\n",
    "            context_end = idx - 1\n",
    "\n",
    "            supp_para_start_positions.append([])\n",
    "            for supp_para_idx in range(len(supp_paras[sample_idx])):\n",
    "                supp_para = supp_paras[sample_idx][supp_para_idx]\n",
    "                supp_para_start_char = supp_para[0]\n",
    "                supp_para_end_char = supp_para[1]\n",
    "\n",
    "                if offset[context_start][0] > supp_para_start_char or offset[context_end][1] < supp_para_end_char:\n",
    "                    continue\n",
    "                else:\n",
    "                    idx1 = context_start\n",
    "                    while idx1 <= context_end and offset[idx1][0] <= supp_para_start_char:\n",
    "                        idx1 += 1\n",
    "                    supp_para_start_positions[-1].append(idx1 - 1)\n",
    "        \n",
    "        title_positions = []\n",
    "        for i, supp in zip([inputs['input_ids'][ii] for ii in ex_indices], supp_para_start_positions):\n",
    "            title_positions.append([0 if j not in supp else 1 for j in np.where(np.array(i) == PARA_MARKER_token)[0]])\n",
    "            title_positions[-1] = pad_and_drop_duplicates(start_positions=title_positions[-1], \n",
    "                                                          max_num_answers=max_num_paragraphs)\n",
    "\n",
    "        concat_input_ids = []\n",
    "        concat_offset_mapping = []\n",
    "        for ii in ex_indices:\n",
    "            concat_input_ids += inputs['input_ids'][ii]\n",
    "            concat_offset_mapping += inputs['offset_mapping'][ii]\n",
    "\n",
    "        titles = np.where(np.array(concat_input_ids) == PARA_MARKER_token)[0]\n",
    "        np_titles_concat_offset_mapping = np.array(concat_offset_mapping)[titles]\n",
    "        titles_offsets_unique = sorted(np.unique(np_titles_concat_offset_mapping.tolist(), axis=0).tolist())\n",
    "        concat_titles_ids = [titles_offsets_unique.index(list(i)) for i in np_titles_concat_offset_mapping]\n",
    "      \n",
    "        batch_concat_titles_ids.append(concat_titles_ids)\n",
    "        batch_title_positions.append(title_positions)    \n",
    "\n",
    "    inputs.update({'context_start_id': ex_context_start_id})\n",
    "    inputs.update({'context_end_id': ex_context_end_id})\n",
    "    inputs.update({\"supp_para_labels\": batch_title_positions})\n",
    "    inputs.update({\"concat_titles_ids\": batch_concat_titles_ids})\n",
    "\n",
    "    rearranged_inps = []\n",
    "    rearranged_masks = []\n",
    "    rearranged_offset_mapping = []\n",
    "\n",
    "    for ex_id in examples[\"id\"]:\n",
    "        ex_indices = np.where(np.array(raw_ids) == ex_id)[0].tolist()\n",
    "        rearranged_inps.append([inputs['input_ids'][i] for i in ex_indices])\n",
    "        rearranged_masks.append([inputs['attention_mask'][i] for i in ex_indices])\n",
    "        rearranged_offset_mapping.append([inputs['offset_mapping'][i] for i in ex_indices])\n",
    "\n",
    "    inputs.update({'input_ids': rearranged_inps})\n",
    "    inputs.update({'attention_mask': rearranged_masks})\n",
    "    inputs.update({'offset_mapping': rearranged_offset_mapping})\n",
    "\n",
    "    return inputs\n",
    "\n",
    "mem_val_dataset = val_examples.map(\n",
    "    preprocess_longformer_validation_examples,\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    remove_columns=val_examples.column_names,\n",
    ")\n",
    "mem_val_dataset.save_to_disk('../musique_preprocessed_val_examples_512_allanai_style_multitask_stride20_one_doc_batched_without_zero_answer_pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d577ac3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_venv",
   "language": "python",
   "name": "hf_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
